# YOLOv2 Object Detection Tutorial

## Introduction
You only look once (YOLO) is a state-of-the-art, real-time object detection algorithm.  
The algorithm was published by Redmon et al. in 2016 via the following publications:
[YOLOv1](https://arxiv.org/abs/1506.02640),
[YOLOv2](https://arxiv.org/abs/1612.08242).  
The same author has already released YOLOv3, and some experimental tiny YOLO networks. This tutorial focuses on different YOLOv2 networks.
This application requires more than just simple classification. The task here is to detect the presence of objects, and localize them within a frame. 
Please refer to the papers for full algorithm details, and/or watch [this.](https://www.youtube.com/watch?v=9s_FpMpdYW8). 
In this tutorial, the network was trained on the 80 class [COCO dataset.](http://cocodataset.org/#home)

## Background
The authors of the YOLO papers used their own programming framework called "Darknet" for research, and development. The framework is written in C, and was [open sourced.](https://github.com/pjreddie/darknet) Additionally, they host documentation, and pretrained weights [here.](https://pjreddie.com/darknet/yolov2/) Currently, the Darknet framework is not supported by Xilinx's ML Suite. Additionally, there are some aspects of the YOLOv2 network that are not supported by the Hardware Accelerator, such as the reorg layer. For these reasons we are sharing original and modified versions of YOLOv2 network. The inference using original YOLOv2 version is acheived by running reorg layer in software. The modified version of the YOLOv2 network was obtained by  replacing unsuppored layers with supported layers, retraining this modified network on Darknet, and converting the model to caffe. In this tutorial we will run the network accelerated on an FPGA using 8b quantized weights and a hardware kernel implementing a 96x32 systolic array with 9MB of image RAM. All convolutions/pools/leaky-ReLU are accelerated on the FPGA fabric, while the final sigmoid, softmax, and non-max suppression functions are executed on the CPU. 

## Running the Application
Xilinx has provided a demo application showing the different YOLOv2 networks can be run in different modes to cater to differnt use cases and speed requirements
Xilinx has provided a demo application showing how YOLOv2 can be ran "end to end", meaning we will run all of the required offline, and online software to get some example results.
[yolo.py](./yolo.py) is the top level python module where you will see how the compiler, quantizer, and xyolo module are invoked.   
[configs.py](./configs.py) is a configuration file used to modify the desired run configuration. We are supporting 608x608/224x224 images, 16b/8b quantization.  
[xyolo.py](./xyolo.py) is a python class meant to be reusable, but it also demonstrates how to use the PYXDNN in a custom application. It provides a YOLO detect method.  
[run.sh](./run.sh) is a bash script used to run some enviornment setup, and launch the demo app  

 To run:
 1. Connect to F1 or Local Hardware
 
 2. setup the docker/comntainer. Please refer to [container.md](../../docs/container.md) for details on setting up the container
 
 3. Download the xilinx trained models from Xilinx.com, save as models at the root of this repo 
 
 4. `cd MLsuite/apps/yolo`
 
 5. Familiarize yourself with the script usage by:  
  `./run.sh -h`  
  The key parameters are:
    - -p `platform` Valid values are `alveo-u200`, `alveo-u250`, `aws`, `nimbix`, `1525`, `1525-ml` and `gpu` 
    - -t `test` - Currently valid value are  `test_detect` , `streaming_detect` and `streaming_detect_benchmark'
    - -k `kernel config` - Valid values are `large` or `v3` - Used to select overlaybins
    - -b `quantization precision` - Valid values are `16` or `8` - corresponding to INT16 or INT8
      - Note: XDNNv3 only supports 8 which is also the default precision (INT8)
    - -g `calclulate mAP score` - to calcluate mAP score specify the folder containing the labels(txt) files for set of images given with -d option
    - -d `images` - Directory of images
    - -c `select perfromance modes` - throghput and latency are the 2 modes that serve different perfromance requirments and are discribed in the [runtime-modes-features.md](../../docs/runtime-modes-features.md) document 
    - -cw `net weights dumped by complier` - to use pre-generated weights, specify folder containing weights and biases as txt files
    - -cn `net.cmds.json file generated by compler` - to use pre generated compiler file, specify schedule generated by compiler
    - -cq `net_quant.json file generated by quantizer` - to use pre generated quantizer file, specify quantizer parameters generated by quantizer
    - -m `select model and image resolution` - specific model and resolution of the network to run. options are
       - `yolo_v2_224` : select modified YOLOv2 network where reorg is replaced with maxpool and LeakyReLU is replaced with ReLU. Change the suffix 224 to 408 or 608 for the other 2 supported input resolutions
       - `yolo_v2_standard_224` : select original YOLOv2 network where reorg is run in software. Change the suffix 224 to 408 or 608 for the other 2 supported input resolutions
       - `yolo_v2_prelu_224` : select modified YOLOv2 network where reorg is replaced with maxpool. Change the suffix 224 to 408 or 608 for the other 2 supported input resolutions
       - `yolo_v2_tiny_224` : select tiny YOLOv2 network darknet stype maxpool and rest of the layers are run in software. Change the suffix 224 to 408 or 608 for the other 2 supported input resolutions
    
      
## Example Invocations
1. Object detection on a set of images on alveo-u200, with yolo-v2 input 3x224x224, with XDNNv3 in throghput mode:
    ```sh
    $ ./run.sh -p alveo-u200 -t test_detect -k v3 -b 8 -m yolo_v2_224 --compilerOpt throughput
    ```
2. Object detection on a set of images on alveo-u200, with yolo-v2 input 3x224x224, with XDNNv3 in latency mode:
    ```sh
    $ ./run.sh -p alveo-u200 -t test_detect -k v3 -b 8 -m yolo_v2_224 --compilerOpt latency
    ```
3. Object detection on a set of images on alveo-u200, with yolo-v2 input 3x224x224, with 8-bit XDNNv2 large kernel:
    ```sh
    $ ./run.sh -p alveo-u200 -t test_detect -k large -b 8 -m yolo_v2_224
    ```
4. Object detection on a set of images on alveo-u200, with yolo-v2 input 3x608x608, with 8-bit XDNNv2 large kernel:
    ```sh
    $ ./run.sh -p alveo-u200 -t test_detect -k large -b 8 -m yolo_v2_608
    ```
5. Object detection and compute mAP score on a set of images on alveo-u200, with tiny yolo-v2 input 3x608x608, with 8-bit XDNNv2 large kernel:
    Prepare Data 
    ```
    Download Images and lables
    wget -c https://pjreddie.com/media/files/val2014.zip
    unzip -q val2014.zip
    wget -c https://pjreddie.com/media/files/coco/labels.tgz
    tar xzf labels.tgz
    ```

    ```sh
    $ ./run.sh -p alveo-u200 -t test_detect -k large -b 8 -m yolo_v2_tiny_608 -g labels/val2014/ -d val2014/
    ```
6. Object detection in streaming mode on alveo-u200, with tiny yolo-v2 input 3x608x608, with 8-bit XDNNv2 large kernel:
    ```sh
    $ ./run.sh -p alveo-u200 -t streaming_detect -k large -b 8 -m yolo_v2_tiny_608 -cw yolo_v2_tiny.caffemodel_data/ -cn yolo_v2_tiny.cmds.json -cq yolo_v2_8bit_deploy.json
    ```
7. Object detection in benchmarking mode on alveo-u200, with tiny yolo-v2 input 3x608x608, with 8-bit XDNNv2 large kernel:
    ```sh
    $ ./run.sh -p alveo-u200 -t streaming_detect_benchmark -k large -b 8 -m yolo_v2_tiny_608 -cw yolo_v2_tiny.caffemodel_data/ -cn yolo_v2_tiny.cmds.json -cq yolo_v2_8bit_deploy.json
    ```
8. Object detection on cpu, with tiny yolo-v2 input 3x608x608, with 8-bit precison:
    ```sh
    $ ./run.sh -p cpu -t test_detect -m yolo_v2_tiny_608 
    ```


Refer to the Using Anaconda on AWS instructions located [here][]. 

 Upon success, you will see several bounding box predictions printed for the images in the 'out' directory

Note: After the first initial run, it is possible to run the demo with `python yolo.py` the run.sh script is setting up some key env variables, and building the non-max suppression binary. However, that only needs to be done once, in a shell

[here]: ../../docs/tutorials/start-anaconda.md
